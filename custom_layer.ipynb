{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist #to import our dataset\n",
    "from tensorflow.keras.models import Sequential, Model # imports our type of network\n",
    "from tensorflow.keras.layers import Dense, Flatten,Conv2D, Input, Lambda, Layer, ReLU # imports our layers we want to use\n",
    "\n",
    "from tensorflow.python.keras.losses import categorical_crossentropy #loss function\n",
    "from tensorflow.python.keras.optimizers import Adam, SGD #optimisers\n",
    "from tensorflow.python.keras.utils import to_categorical #some function for data preparation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate\n",
    "\n",
    "def rotate_img(img, angle):\n",
    "    #angle in degree\n",
    "    return rotate(img, angle)\n",
    "\n",
    "# number of classes of the mnist dataset\n",
    "num_classes = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# load mnist data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#prepare the data\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "example=np.reshape([rotate_img(x_train[0],0),rotate_img(x_train[0],90),rotate_img(x_train[0],180),rotate_img(x_train[0],270)],(4,28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create randomly rotation angles of 0,90,180 or 270 degrees for each image\n",
    "angles_train = np.random.randint(0,4, size = x_train.shape[0])*90\n",
    "angles_test = np.random.randint(0,4, size = x_test.shape[0])*90\n",
    "\n",
    "#now rotate each image by the respective angle\n",
    "for i in range(x_train.shape[0]):\n",
    "    x_train[i] = rotate_img(x_train[i],angles_train[i])\n",
    "for i in range(x_test.shape[0]):\n",
    "    x_test[i] = rotate_img(x_test[i],angles_test[i])\n",
    "    \n",
    "x_train=x_train.reshape(len(x_train),28,28,1)\n",
    "x_test=x_test.reshape(len(x_test),28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_142 (InputLayer)       [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "lambda_47 (Lambda)           (None, 28, 28)            0         \n",
      "_________________________________________________________________\n",
      "lambda_48 (Lambda)           (None, 28, 28)            0         \n",
      "=================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_network = Input(shape=(28,28))\n",
    "x = Lambda(lambda x: tf.einsum('kij->kji',x))(input_network) #transpose\n",
    "x = Lambda(lambda x: tf.reverse(x,[-2]))(x) #reverse ordering\n",
    "model= Model(input_network,outputs=x)\n",
    "opt = Adam(lr=0.0001)\n",
    "model.compile(loss=categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(x_train[:3,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADkJJREFUeJzt3X+MHPV5x/HP4+NsqG0Mh8P1YhDGjiFyrWDoyoBCAigNJW6EQUkJqEpNizhKIXLUVKrrVAmVqpRWAZoogcpJTExFHLciDlaEqInTYjkyDodl/CMmxdiHsHO2cQ3BhHL22U//uDE64Oa7693ZnT2e90s63e48MzuP1v7c7Ox3d77m7gIQz7iyGwBQDsIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCoU1q5s/E2wU/VxFbuEgjlLf1WR3zQalm3ofCb2bWSviGpQ9J33f2e1PqnaqIutU80sksACRt9bc3r1v2y38w6JH1b0qckzZZ0s5nNrvfxALRWI+f88yTtdPdd7n5E0g8lLSimLQDN1kj4p0l6ecT9PdmydzCzXjPrM7O+oxpsYHcAitT0d/vdfam7V9y90qkJzd4dgBo1Ev69ks4dcf+cbBmAMaCR8D8jaZaZnW9m4yXdJGl1MW0BaLa6h/rcfcjM7pL0nxoe6lvm7tsL6wxAUzU0zu/uj0t6vKBeALQQH+8FgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiColl66G/F0dJ+dW3v+785PbusdnqwvunJNsn7nGS8m6424YPUd6fodv2javovCkR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmKcH0kdZ0xJ1p//hw8n6+uuuze31t1xWl09FWHN/6Wniv/azvnJ+vQfHy+ynVJw5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBoa5zezfkmHJR2TNOTulSKaQnHGTZ6crB/87Jxk/ZI7Nifrj017oEoHzRvLX3G4O1l/cPeVubWuP/9tcttJA7uq7L1avf0V8SGfq939YAGPA6CFeNkPBNVo+F3SGjN71sx6i2gIQGs0+rL/Cnffa2ZnS3rSzJ5393UjV8j+KPRK0qn6nQZ3B6AoDR353X1v9vuApFWS5o2yzlJ3r7h7pVMTGtkdgALVHX4zm2hmk0/clnSNpG1FNQaguRp52d8taZWZnXicH7j7E4V0BaDp6g6/u++SdFGBvSDPuI5k+dAt7znbetvlf9mX3HZ1z7fqaqkI//rajGR95VeuTdanPDuQrJ/en3/d/qHkljEw1AcERfiBoAg/EBThB4Ii/EBQhB8Iikt3t4FXF16erF+56Olk/Wvd5Q3XPfrG1GT97pU35dZmPvTr5LYTd29M1hmuawxHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IinH+Fuj8755k/Wcfuj9Zn2CdRbbzDuveGp+s3/bzhcn6Bd88kqyf17cht8Y4fbk48gNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIzzF6HKpbUv69qdrDc6jv+ZnX+UW9u2eXpy25n/8VayPmv9pmTdk1W0M478QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBU1XF+M1sm6dOSDrj7nGxZl6SVkqZL6pd0o7u/2rw229ubCyrJ+he6/qXKI6S/U/+mp78zP3jlvtzaLOXXEFstR/7vS3r3ROmLJa1191mS1mb3AYwhVcPv7uskHXrX4gWSlme3l0u6vuC+ADRZvef83e4+kN3eJ6m7oH4AtEjDb/i5uyvxEW8z6zWzPjPrO6rBRncHoCD1hn+/mfVIUvb7QN6K7r7U3SvuXunUhDp3B6Bo9YZ/taQTl3VdKOmxYtoB0CpVw29mKyRtkHShme0xs1sl3SPpk2b2gqQ/yO4DGENs+JS9NU63Lr/UPtGy/bWLKevPStYfOX9Nsn5cx5P1S/9pUW7tgyt3Jrc9tj/3jA1j0EZfq9f9kNWyLp/wA4Ii/EBQhB8IivADQRF+ICjCDwTFUF8LdMyakaz/6qtTkvXnr/5u3fu+edcfJuuvLz4nWbefb65732g9hvoAVEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzt8GTpkxPVl/4baeZH37n36r7n3vHkpP0f0nf//XyXrXsg117xvFY5wfQFWEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4/xjgaWHbV/+8uX5m178m+S2//iRVcn6741PX9r7rutuS9aPP7cjWUexGOcHUBXhB4Ii/EBQhB8IivADQRF+ICjCDwRVdZzfzJZJ+rSkA+4+J1t2t6TbJL2SrbbE3R+vtjPG+dvP4c9dlqw/dd+3k/U/eyn973lofv7/r2OvpT+DgJNX9Dj/9yVdO8ry+919bvZTNfgA2kvV8Lv7OkmHWtALgBZq5Jz/LjPbYmbLzOzMwjoC0BL1hv9BSTMlzZU0IOnevBXNrNfM+sys76gG69wdgKLVFX533+/ux9z9uKTvSJqXWHepu1fcvdKpCfX2CaBgdYXfzEZeTvYGSduKaQdAq5xSbQUzWyHpKklTzWyPpK9KusrM5kpySf2Sbm9ijwCagO/zRzeuI1n+wPrJyfpD561N1i/4yV/k125/JrktTh7f5wdQFeEHgiL8QFCEHwiK8ANBEX4gqKrj/HifO34sWd7+yOz09kvSQ32dpx/JL1a5JHnHWV3J+rGD/5usI40jPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExTj/GLB7xUXJ+vRv5tdsw3PJbTvOmJKsX9f7VLJezfaPL8utXbTkC8ltj33kjWT99CcuSNa7HtqQrEfHkR8IivADQRF+ICjCDwRF+IGgCD8QFOEHguLS3WPAAy+tT9anjMv/XvwtL342ue2MSQeT9Xt7nk7Wy7TicHeyvvLqSm5taGBf0e20BS7dDaAqwg8ERfiBoAg/EBThB4Ii/EBQhB8Iqur3+c3sXEkPS+qW5JKWuvs3zKxL0kpJ0yX1S7rR3V9tXqvNZZU5yfrg1NNya4enNXZZhKGJ6WHZno5fJOudlj/N9qpZP6mrp7Hg9099OVl/+MLrcmsd79Nx/pNRy5F/SNKX3H22pMsk3WlmsyUtlrTW3WdJWpvdBzBGVA2/uw+4+6bs9mFJOyRNk7RA0vJsteWSrm9WkwCKd1Ln/GY2XdLFkjZK6nb3gay0T8OnBQDGiJrDb2aTJD0q6Yvu/vrImg9/QWDULwmYWa+Z9ZlZ31ENNtQsgOLUFH4z69Rw8B9x9x9li/ebWU9W75F0YLRt3X2pu1fcvdKpCUX0DKAAVcNvZibpe5J2uPt9I0qrJS3Mbi+U9Fjx7QFollrGqD4q6fOStprZ5mzZEkn3SPp3M7tV0kuSbmxOi8V484ZLk/XZf7slWV/c/WRu7ZxT8ocBJemqrX+crH9l5hPJemoor2wfe+5zyfqh30zMrZ32dH5Nkj741GvJug0eTdY7dmxK1qOrGn53Xy8pbyCaL+cDYxSf8AOCIvxAUIQfCIrwA0ERfiAowg8EFWaK7l9/LP212VU9P03Wp3ZMyq1d0pce6/7dvzqSrN/3wDXJ+qIXz07WO97M/xt+4df7k9s26oxX0o8/ZWio7sc+XveWqAVHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8Iiim6gfcRpugGUBXhB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFU1/GZ2rpn9l5n90sy2m9mibPndZrbXzDZnP/Ob3y6AotQyaceQpC+5+yYzmyzpWTN7Mqvd7+5fb157AJqlavjdfUDSQHb7sJntkDSt2Y0BaK6TOuc3s+mSLpa0MVt0l5ltMbNlZnZmzja9ZtZnZn1HNdhQswCKU3P4zWySpEclfdHdX5f0oKSZkuZq+JXBvaNt5+5L3b3i7pVOTSigZQBFqCn8Ztap4eA/4u4/kiR33+/ux9z9uKTvSJrXvDYBFK2Wd/tN0vck7XD3+0Ys7xmx2g2SthXfHoBmqeXd/o9K+rykrWa2OVu2RNLNZjZXkkvql3R7UzoE0BS1vNu/XtJo1wF/vPh2ALQKn/ADgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EZe7eup2ZvSLppRGLpko62LIGTk679taufUn0Vq8iezvP3T9Qy4otDf97dm7W5+6V0hpIaNfe2rUvid7qVVZvvOwHgiL8QFBlh39pyftPadfe2rUvid7qVUpvpZ7zAyhP2Ud+ACUpJfxmdq2Z/crMdprZ4jJ6yGNm/Wa2NZt5uK/kXpaZ2QEz2zZiWZeZPWlmL2S/R50mraTe2mLm5sTM0qU+d+0243XLX/abWYek/5H0SUl7JD0j6WZ3/2VLG8lhZv2SKu5e+piwmX1c0huSHnb3Odmyf5Z0yN3vyf5wnunuf9Mmvd0t6Y2yZ27OJpTpGTmztKTrJd2iEp+7RF83qoTnrYwj/zxJO919l7sfkfRDSQtK6KPtufs6SYfetXiBpOXZ7eUa/s/Tcjm9tQV3H3D3Tdntw5JOzCxd6nOX6KsUZYR/mqSXR9zfo/aa8tslrTGzZ82st+xmRtGdTZsuSfskdZfZzCiqztzcSu+aWbptnrt6ZrwuGm/4vdcV7n6JpE9JujN7eduWfPicrZ2Ga2qaublVRplZ+m1lPnf1znhdtDLCv1fSuSPun5Mtawvuvjf7fUDSKrXf7MP7T0ySmv0+UHI/b2unmZtHm1labfDctdOM12WE/xlJs8zsfDMbL+kmSatL6OM9zGxi9kaMzGyipGvUfrMPr5a0MLu9UNJjJfbyDu0yc3PezNIq+blruxmv3b3lP5Lma/gd/xclfbmMHnL6miHpuexne9m9SVqh4ZeBRzX83sitks6StFbSC5J+KqmrjXr7N0lbJW3RcNB6SurtCg2/pN8iaXP2M7/s5y7RVynPG5/wA4LiDT8gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0H9PxrTeni5vkdpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's check our result:\n",
    "plt.imshow(x_train[0,:,:,0])\n",
    "plt.show()\n",
    "plt.imshow(pred[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 100)               78500     \n",
      "=================================================================\n",
      "Total params: 78,500\n",
      "Trainable params: 78,500\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_network = Input(shape=(28*28,))\n",
    "x = Dense(100)(input_network)\n",
    "m_bare= Model(input_network,outputs=x)\n",
    "#opt = Adam(lr=0.0001)\n",
    "#model.compile(loss=categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
    "m_bare.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_outputs):\n",
    "        super(MyDenseLayer, self).__init__()\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]),self.num_outputs])\n",
    "        self.bias = self.add_weight(\"bias\", shape=[self.num_outputs])\n",
    "\n",
    "    #def call(self, input):\n",
    "    #    return tf.matmul(input, self.kernel)    \n",
    "        \n",
    "    def call(self, input):\n",
    "        #wx=tf.matmul(input, self.kernel)\n",
    "        wx=tf.einsum('ij,jn->in',input, self.kernel)\n",
    "        return tf.add(wx, self.bias)\n",
    "\n",
    "layer = MyDenseLayer(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_141 (InputLayer)       [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_57 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "my_dense_layer_19 (MyDenseLa (None, 200)               157000    \n",
      "=================================================================\n",
      "Total params: 157,000\n",
      "Trainable params: 157,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#just a dense layer\n",
    "\n",
    "layer = MyDenseLayer(200)\n",
    "\n",
    "input_network = Input(shape=(28,28))\n",
    "x=Flatten()(input_network)\n",
    "x=layer(x)\n",
    "m_all= Model(input_network,outputs=x)\n",
    "\n",
    "#opt = Adam(lr=0.0001)\n",
    "#m_all.compile(loss=categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
    "m_all.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to get to this function\n",
    "# got to work a convolutional layer as a custom layer\n",
    "# got to work 4 convolutional layers with the same weight\n",
    "# implement the transpose and reverse ordering on the weights (check dimension of the weights model.layers[1].get_weights())\n",
    "\n",
    "class RotatedConv(tf.keras.layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, strides=None, padding=None, activation=None,use_bias=False, **kwargs):\n",
    "        super(RotatedConv, self).__init__()\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.conv = tf.keras.layers.Conv2D(\n",
    "           self.filters,\n",
    "           self.kernel_size,\n",
    "           padding=self.padding,use_bias=self.use_bias\n",
    "       )\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        x1 = self.conv(inputs)\n",
    "        w2=self.conv.weights[0]\n",
    "        w2=tf.einsum('ijlm->jilm',w2)\n",
    "        w2=tf.reverse(w2,[-4])\n",
    "        w3=w2\n",
    "        w3=tf.einsum('ijlm->jilm',w3)\n",
    "        w3=tf.reverse(w3,[-4])\n",
    "        w4=w3\n",
    "        w4=tf.einsum('ijlm->jilm',w4)\n",
    "        w4=tf.reverse(w4,[-4])\n",
    "        x2 = tf.nn.conv2d(\n",
    "                inputs,w2, \n",
    "                padding=self.padding,\n",
    "                strides=self.strides)\n",
    "        x3 = tf.nn.conv2d(\n",
    "                inputs,\n",
    "                w3, \n",
    "                padding=self.padding,\n",
    "                strides=self.strides)\n",
    "        x4 = tf.nn.conv2d(\n",
    "                inputs,\n",
    "                w4, \n",
    "                padding=self.padding,\n",
    "                strides=self.strides)\n",
    "        return Lambda(lambda x: tf.stack([x[0],x[1],x[2],x[3]],axis=-1))([x1,x2,x3,x4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_144 (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "rotated_conv_3 (RotatedConv) (None, 1, 1, 1, 4)        784       \n",
      "_________________________________________________________________\n",
      "flatten_59 (Flatten)         (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 784\n",
      "Trainable params: 784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.1143059 , -0.09360655, -0.2583987 , -0.23722228],\n",
       "       [-0.23722228, -0.11430588, -0.09360659, -0.25839874],\n",
       "       [-0.25839877, -0.23722225, -0.11430587, -0.09360654],\n",
       "       [-0.09360658, -0.25839877, -0.23722225, -0.11430591]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_network = Input(shape=(28,28,1))\n",
    "#x=Flatten()(input_network)\n",
    "x=RotatedConv(1,(28,28),padding='VALID')(input_network)\n",
    "x=Flatten()(x)\n",
    "#x=ReLU()(x)\n",
    "m_all= Model(input_network,outputs=x)\n",
    "\n",
    "#opt = Adam(lr=0.0001)\n",
    "#m_all.compile(loss=categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
    "m_all.summary()\n",
    "\n",
    "\n",
    "m_all.predict(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAADkJJREFUeJzt3X+MHPV5x/HP4+NsqG0Mh8P1YhDGjiFyrWDoyoBCAigNJW6EQUkJqEpNizhKIXLUVKrrVAmVqpRWAZoogcpJTExFHLciDlaEqInTYjkyDodl/CMmxdiHsHO2cQ3BhHL22U//uDE64Oa7693ZnT2e90s63e48MzuP1v7c7Ox3d77m7gIQz7iyGwBQDsIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCoU1q5s/E2wU/VxFbuEgjlLf1WR3zQalm3ofCb2bWSviGpQ9J33f2e1PqnaqIutU80sksACRt9bc3r1v2y38w6JH1b0qckzZZ0s5nNrvfxALRWI+f88yTtdPdd7n5E0g8lLSimLQDN1kj4p0l6ecT9PdmydzCzXjPrM7O+oxpsYHcAitT0d/vdfam7V9y90qkJzd4dgBo1Ev69ks4dcf+cbBmAMaCR8D8jaZaZnW9m4yXdJGl1MW0BaLa6h/rcfcjM7pL0nxoe6lvm7tsL6wxAUzU0zu/uj0t6vKBeALQQH+8FgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiColl66G/F0dJ+dW3v+785PbusdnqwvunJNsn7nGS8m6424YPUd6fodv2javovCkR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgmKcH0kdZ0xJ1p//hw8n6+uuuze31t1xWl09FWHN/6Wniv/azvnJ+vQfHy+ynVJw5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBoa5zezfkmHJR2TNOTulSKaQnHGTZ6crB/87Jxk/ZI7Nifrj017oEoHzRvLX3G4O1l/cPeVubWuP/9tcttJA7uq7L1avf0V8SGfq939YAGPA6CFeNkPBNVo+F3SGjN71sx6i2gIQGs0+rL/Cnffa2ZnS3rSzJ5393UjV8j+KPRK0qn6nQZ3B6AoDR353X1v9vuApFWS5o2yzlJ3r7h7pVMTGtkdgALVHX4zm2hmk0/clnSNpG1FNQaguRp52d8taZWZnXicH7j7E4V0BaDp6g6/u++SdFGBvSDPuI5k+dAt7znbetvlf9mX3HZ1z7fqaqkI//rajGR95VeuTdanPDuQrJ/en3/d/qHkljEw1AcERfiBoAg/EBThB4Ii/EBQhB8Iikt3t4FXF16erF+56Olk/Wvd5Q3XPfrG1GT97pU35dZmPvTr5LYTd29M1hmuawxHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8IinH+Fuj8755k/Wcfuj9Zn2CdRbbzDuveGp+s3/bzhcn6Bd88kqyf17cht8Y4fbk48gNBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIzzF6HKpbUv69qdrDc6jv+ZnX+UW9u2eXpy25n/8VayPmv9pmTdk1W0M478QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxBU1XF+M1sm6dOSDrj7nGxZl6SVkqZL6pd0o7u/2rw229ubCyrJ+he6/qXKI6S/U/+mp78zP3jlvtzaLOXXEFstR/7vS3r3ROmLJa1191mS1mb3AYwhVcPv7uskHXrX4gWSlme3l0u6vuC+ADRZvef83e4+kN3eJ6m7oH4AtEjDb/i5uyvxEW8z6zWzPjPrO6rBRncHoCD1hn+/mfVIUvb7QN6K7r7U3SvuXunUhDp3B6Bo9YZ/taQTl3VdKOmxYtoB0CpVw29mKyRtkHShme0xs1sl3SPpk2b2gqQ/yO4DGENs+JS9NU63Lr/UPtGy/bWLKevPStYfOX9Nsn5cx5P1S/9pUW7tgyt3Jrc9tj/3jA1j0EZfq9f9kNWyLp/wA4Ii/EBQhB8IivADQRF+ICjCDwTFUF8LdMyakaz/6qtTkvXnr/5u3fu+edcfJuuvLz4nWbefb65732g9hvoAVEX4gaAIPxAU4QeCIvxAUIQfCIrwA0Exzt8GTpkxPVl/4baeZH37n36r7n3vHkpP0f0nf//XyXrXsg117xvFY5wfQFWEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4/xjgaWHbV/+8uX5m178m+S2//iRVcn6741PX9r7rutuS9aPP7cjWUexGOcHUBXhB4Ii/EBQhB8IivADQRF+ICjCDwRVdZzfzJZJ+rSkA+4+J1t2t6TbJL2SrbbE3R+vtjPG+dvP4c9dlqw/dd+3k/U/eyn973lofv7/r2OvpT+DgJNX9Dj/9yVdO8ry+919bvZTNfgA2kvV8Lv7OkmHWtALgBZq5Jz/LjPbYmbLzOzMwjoC0BL1hv9BSTMlzZU0IOnevBXNrNfM+sys76gG69wdgKLVFX533+/ux9z9uKTvSJqXWHepu1fcvdKpCfX2CaBgdYXfzEZeTvYGSduKaQdAq5xSbQUzWyHpKklTzWyPpK9KusrM5kpySf2Sbm9ijwCagO/zRzeuI1n+wPrJyfpD561N1i/4yV/k125/JrktTh7f5wdQFeEHgiL8QFCEHwiK8ANBEX4gqKrj/HifO34sWd7+yOz09kvSQ32dpx/JL1a5JHnHWV3J+rGD/5usI40jPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ExTj/GLB7xUXJ+vRv5tdsw3PJbTvOmJKsX9f7VLJezfaPL8utXbTkC8ltj33kjWT99CcuSNa7HtqQrEfHkR8IivADQRF+ICjCDwRF+IGgCD8QFOEHguLS3WPAAy+tT9anjMv/XvwtL342ue2MSQeT9Xt7nk7Wy7TicHeyvvLqSm5taGBf0e20BS7dDaAqwg8ERfiBoAg/EBThB4Ii/EBQhB8Iqur3+c3sXEkPS+qW5JKWuvs3zKxL0kpJ0yX1S7rR3V9tXqvNZZU5yfrg1NNya4enNXZZhKGJ6WHZno5fJOudlj/N9qpZP6mrp7Hg9099OVl/+MLrcmsd79Nx/pNRy5F/SNKX3H22pMsk3WlmsyUtlrTW3WdJWpvdBzBGVA2/uw+4+6bs9mFJOyRNk7RA0vJsteWSrm9WkwCKd1Ln/GY2XdLFkjZK6nb3gay0T8OnBQDGiJrDb2aTJD0q6Yvu/vrImg9/QWDULwmYWa+Z9ZlZ31ENNtQsgOLUFH4z69Rw8B9x9x9li/ebWU9W75F0YLRt3X2pu1fcvdKpCUX0DKAAVcNvZibpe5J2uPt9I0qrJS3Mbi+U9Fjx7QFollrGqD4q6fOStprZ5mzZEkn3SPp3M7tV0kuSbmxOi8V484ZLk/XZf7slWV/c/WRu7ZxT8ocBJemqrX+crH9l5hPJemoor2wfe+5zyfqh30zMrZ32dH5Nkj741GvJug0eTdY7dmxK1qOrGn53Xy8pbyCaL+cDYxSf8AOCIvxAUIQfCIrwA0ERfiAowg8EFWaK7l9/LP212VU9P03Wp3ZMyq1d0pce6/7dvzqSrN/3wDXJ+qIXz07WO97M/xt+4df7k9s26oxX0o8/ZWio7sc+XveWqAVHfiAowg8ERfiBoAg/EBThB4Ii/EBQhB8Iiim6gfcRpugGUBXhB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBFU1/GZ2rpn9l5n90sy2m9mibPndZrbXzDZnP/Ob3y6AotQyaceQpC+5+yYzmyzpWTN7Mqvd7+5fb157AJqlavjdfUDSQHb7sJntkDSt2Y0BaK6TOuc3s+mSLpa0MVt0l5ltMbNlZnZmzja9ZtZnZn1HNdhQswCKU3P4zWySpEclfdHdX5f0oKSZkuZq+JXBvaNt5+5L3b3i7pVOTSigZQBFqCn8Ztap4eA/4u4/kiR33+/ux9z9uKTvSJrXvDYBFK2Wd/tN0vck7XD3+0Ys7xmx2g2SthXfHoBmqeXd/o9K+rykrWa2OVu2RNLNZjZXkkvql3R7UzoE0BS1vNu/XtJo1wF/vPh2ALQKn/ADgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8EZe7eup2ZvSLppRGLpko62LIGTk679taufUn0Vq8iezvP3T9Qy4otDf97dm7W5+6V0hpIaNfe2rUvid7qVVZvvOwHgiL8QFBlh39pyftPadfe2rUvid7qVUpvpZ7zAyhP2Ud+ACUpJfxmdq2Z/crMdprZ4jJ6yGNm/Wa2NZt5uK/kXpaZ2QEz2zZiWZeZPWlmL2S/R50mraTe2mLm5sTM0qU+d+0243XLX/abWYek/5H0SUl7JD0j6WZ3/2VLG8lhZv2SKu5e+piwmX1c0huSHnb3Odmyf5Z0yN3vyf5wnunuf9Mmvd0t6Y2yZ27OJpTpGTmztKTrJd2iEp+7RF83qoTnrYwj/zxJO919l7sfkfRDSQtK6KPtufs6SYfetXiBpOXZ7eUa/s/Tcjm9tQV3H3D3Tdntw5JOzCxd6nOX6KsUZYR/mqSXR9zfo/aa8tslrTGzZ82st+xmRtGdTZsuSfskdZfZzCiqztzcSu+aWbptnrt6ZrwuGm/4vdcV7n6JpE9JujN7eduWfPicrZ2Ga2qaublVRplZ+m1lPnf1znhdtDLCv1fSuSPun5Mtawvuvjf7fUDSKrXf7MP7T0ySmv0+UHI/b2unmZtHm1labfDctdOM12WE/xlJs8zsfDMbL+kmSatL6OM9zGxi9kaMzGyipGvUfrMPr5a0MLu9UNJjJfbyDu0yc3PezNIq+blruxmv3b3lP5Lma/gd/xclfbmMHnL6miHpuexne9m9SVqh4ZeBRzX83sitks6StFbSC5J+KqmrjXr7N0lbJW3RcNB6SurtCg2/pN8iaXP2M7/s5y7RVynPG5/wA4LiDT8gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0H9PxrTeni5vkdpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    plt.imshow(example[i].reshape(28,28))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_145 (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "rotated_conv_4 (RotatedConv) (None, 1, 1, 100, 4)      78400     \n",
      "_________________________________________________________________\n",
      "flatten_60 (Flatten)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "re_lu_9 (ReLU)               (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 119,510\n",
      "Trainable params: 119,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_network = Input(shape=(28,28,1))\n",
    "#x=Flatten()(input_network)\n",
    "x=RotatedConv(100,(28,28),padding='VALID')(input_network)\n",
    "x=Flatten()(x)\n",
    "x=ReLU()(x)\n",
    "x=Dense(100,activation='relu')(x)\n",
    "x=Dense(num_classes,activation='softmax')(x)\n",
    "m_all= Model(input_network,outputs=x)\n",
    "\n",
    "opt = Adam(lr=0.0001)\n",
    "m_all.compile(loss=categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
    "m_all.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 21s 353us/sample - loss: 0.6809 - acc: 0.7901 - val_loss: 0.3329 - val_acc: 0.9011\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 30s 504us/sample - loss: 0.2614 - acc: 0.9230 - val_loss: 0.2190 - val_acc: 0.9332\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 24s 402us/sample - loss: 0.1869 - acc: 0.9439 - val_loss: 0.2149 - val_acc: 0.9320\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 24s 398us/sample - loss: 0.1439 - acc: 0.9564 - val_loss: 0.1837 - val_acc: 0.9423\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 19s 313us/sample - loss: 0.1174 - acc: 0.9647 - val_loss: 0.1735 - val_acc: 0.9480\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 19s 311us/sample - loss: 0.0992 - acc: 0.9695 - val_loss: 0.1581 - val_acc: 0.9533\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 19s 312us/sample - loss: 0.0830 - acc: 0.9742 - val_loss: 0.1699 - val_acc: 0.9492\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 19s 318us/sample - loss: 0.0704 - acc: 0.9778 - val_loss: 0.1547 - val_acc: 0.9563\n",
      "Epoch 9/20\n",
      " 4096/60000 [=>............................] - ETA: 15s - loss: 0.0458 - acc: 0.9851"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-325-e9ccc419a70c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=m_all.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_134 (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 1, 1, 100)         78400     \n",
      "_________________________________________________________________\n",
      "flatten_50 (Flatten)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 89,510\n",
      "Trainable params: 89,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_network = Input(shape=(28,28,1))\n",
    "#x=Flatten()(input_network)\n",
    "x=Conv2D(100,(28,28),padding='VALID',use_bias=False)(input_network)\n",
    "x=Flatten()(x)\n",
    "x=ReLU()(x)\n",
    "x=Dense(100,activation='relu')(x)\n",
    "x=Dense(num_classes,activation='softmax')(x)\n",
    "m_2= Model(input_network,outputs=x)\n",
    "\n",
    "opt = Adam(lr=0.0001)\n",
    "m_2.compile(loss=categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
    "m_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.8906 - acc: 0.7203 - val_loss: 0.5749 - val_acc: 0.8280\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.4849 - acc: 0.8547 - val_loss: 0.4109 - val_acc: 0.8750\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 36us/sample - loss: 0.3476 - acc: 0.8960 - val_loss: 0.3360 - val_acc: 0.8993\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 40us/sample - loss: 0.2730 - acc: 0.9186 - val_loss: 0.2909 - val_acc: 0.9119\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.2296 - acc: 0.9306 - val_loss: 0.2731 - val_acc: 0.9151\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1978 - acc: 0.9403 - val_loss: 0.2569 - val_acc: 0.9222\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.1749 - acc: 0.9461 - val_loss: 0.2481 - val_acc: 0.9261\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 3s 43us/sample - loss: 0.1519 - acc: 0.9548 - val_loss: 0.2432 - val_acc: 0.9252\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 2s 39us/sample - loss: 0.1378 - acc: 0.9578 - val_loss: 0.2396 - val_acc: 0.9281\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.1208 - acc: 0.9620 - val_loss: 0.2375 - val_acc: 0.9301\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.1086 - acc: 0.9667 - val_loss: 0.2411 - val_acc: 0.9298\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 3s 46us/sample - loss: 0.0990 - acc: 0.9696 - val_loss: 0.2607 - val_acc: 0.9263\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0851 - acc: 0.9734 - val_loss: 0.2482 - val_acc: 0.9320\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0775 - acc: 0.9755 - val_loss: 0.2416 - val_acc: 0.9330\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0714 - acc: 0.9776 - val_loss: 0.2623 - val_acc: 0.9299\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 0.0614 - acc: 0.9811 - val_loss: 0.2625 - val_acc: 0.9335\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.0574 - acc: 0.9819 - val_loss: 0.2700 - val_acc: 0.9320\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 3s 42us/sample - loss: 0.0519 - acc: 0.9840 - val_loss: 0.2615 - val_acc: 0.9332\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 3s 44us/sample - loss: 0.0448 - acc: 0.9863 - val_loss: 0.2666 - val_acc: 0.9363\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 3s 49us/sample - loss: 0.0407 - acc: 0.9868 - val_loss: 0.2692 - val_acc: 0.9352\n"
     ]
    }
   ],
   "source": [
    "history=m_2.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_135 (InputLayer)       [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 1, 1, 400)         313600    \n",
      "_________________________________________________________________\n",
      "flatten_51 (Flatten)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               40100     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 354,710\n",
      "Trainable params: 354,710\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_network = Input(shape=(28,28,1))\n",
    "#x=Flatten()(input_network)\n",
    "x=Conv2D(400,(28,28),padding='VALID',use_bias=False)(input_network)\n",
    "x=Flatten()(x)\n",
    "x=ReLU()(x)\n",
    "x=Dense(100,activation='relu')(x)\n",
    "x=Dense(num_classes,activation='softmax')(x)\n",
    "m_3= Model(input_network,outputs=x)\n",
    "\n",
    "opt = Adam(lr=0.0001)\n",
    "m_3.compile(loss=categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
    "m_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0157 - acc: 0.9948 - val_loss: 0.2387 - val_acc: 0.9535\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 5s 79us/sample - loss: 0.0222 - acc: 0.9923 - val_loss: 0.2489 - val_acc: 0.9520\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.0146 - acc: 0.9949 - val_loss: 0.2636 - val_acc: 0.9487\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 7s 108us/sample - loss: 0.0182 - acc: 0.9940 - val_loss: 0.2809 - val_acc: 0.9485\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 6s 101us/sample - loss: 0.0165 - acc: 0.9947 - val_loss: 0.2606 - val_acc: 0.9497\n",
      "Epoch 6/20\n",
      "42368/60000 [====================>.........] - ETA: 1s - loss: 0.0104 - acc: 0.9967"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-298-54b0de386f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history=m_3.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"input_146:0\", shape=(?, 28, 28, 1), dtype=float32) at layer \"input_146\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-328-2098a9cbeed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mm_4\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_network\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0;31m# initializing _distribution_strategy here since it is possible to call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# predict on a model without compiling it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m         'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m    161\u001b[0m       \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m       \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     nodes, nodes_by_depth, layers, layers_by_depth = _map_graph_network(\n\u001b[0;32m--> 315\u001b[0;31m         self.inputs, self.outputs)\n\u001b[0m\u001b[1;32m    316\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nodes_by_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes_by_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1848\u001b[0m                              \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                              \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m                              str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1851\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m           \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"input_146:0\", shape=(?, 28, 28, 1), dtype=float32) at layer \"input_146\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "input_network = Input(shape=(28,28,1))\n",
    "x=Dense(100,activation='relu')(x)\n",
    "x=Dense(num_classes,activation='softmax')(x)\n",
    "m_4= Model(input_network,outputs=x)\n",
    "\n",
    "opt = Adam(lr=0.0001)\n",
    "m_4.compile(loss=categorical_crossentropy,optimizer=Adam(), metrics=['accuracy'])\n",
    "m_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.9268 - acc: 0.7178 - val_loss: 0.6018 - val_acc: 0.8231\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.5139 - acc: 0.8547 - val_loss: 0.4398 - val_acc: 0.8721\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.3873 - acc: 0.8896 - val_loss: 0.3640 - val_acc: 0.8923\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.3203 - acc: 0.9075 - val_loss: 0.3270 - val_acc: 0.9034\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.2757 - acc: 0.9202 - val_loss: 0.2971 - val_acc: 0.9090\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.2450 - acc: 0.9289 - val_loss: 0.2942 - val_acc: 0.9126\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.2204 - acc: 0.9366 - val_loss: 0.2786 - val_acc: 0.9167\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.2030 - acc: 0.9420 - val_loss: 0.2688 - val_acc: 0.9191\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.1863 - acc: 0.9456 - val_loss: 0.2618 - val_acc: 0.9224\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.1718 - acc: 0.9499 - val_loss: 0.2652 - val_acc: 0.9199\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.1603 - acc: 0.9528 - val_loss: 0.2558 - val_acc: 0.9243\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.1479 - acc: 0.9572 - val_loss: 0.2514 - val_acc: 0.9255\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.1378 - acc: 0.9601 - val_loss: 0.2610 - val_acc: 0.9237\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 4s 59us/sample - loss: 0.1303 - acc: 0.9615 - val_loss: 0.2547 - val_acc: 0.9254\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.1202 - acc: 0.9653 - val_loss: 0.2509 - val_acc: 0.9273\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 0.1129 - acc: 0.9672 - val_loss: 0.2632 - val_acc: 0.9256\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.1056 - acc: 0.9691 - val_loss: 0.2541 - val_acc: 0.9281\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 5s 78us/sample - loss: 0.0995 - acc: 0.9710 - val_loss: 0.2560 - val_acc: 0.9276\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 0.0945 - acc: 0.9731 - val_loss: 0.2669 - val_acc: 0.9262\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 0.0874 - acc: 0.9749 - val_loss: 0.2706 - val_acc: 0.9234\n"
     ]
    }
   ],
   "source": [
    "history=m_4.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
